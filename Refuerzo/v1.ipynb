{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd44e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simulaci칩n de entrenamiento RL ===\n",
      "State: [19  5 40]  Action: stop  Reward: -1.0\n",
      "State: [40 25 25]  Action: forward  Reward: 0.2\n",
      "State: [40 45 21]  Action: forward  Reward: 0.0\n",
      "State: [44 26 19]  Action: left  Reward: 0.2\n",
      "State: [35  6 39]  Action: forward  Reward: -1.0\n",
      "State: [25 29 48]  Action: right  Reward: 0.0\n",
      "State: [12 32 48]  Action: left  Reward: 0.2\n",
      "State: [37 29 29]  Action: forward  Reward: 0.2\n",
      "State: [46 45 40]  Action: right  Reward: 0.2\n",
      "State: [21 45 38]  Action: forward  Reward: 0.2\n",
      "State: [ 5 50 44]  Action: forward  Reward: -1.0\n",
      "State: [28 20 22]  Action: stop  Reward: 0.2\n",
      "State: [36 24  5]  Action: forward  Reward: -1.0\n",
      "State: [50 41  5]  Action: right  Reward: -1.0\n",
      "State: [ 5  8 30]  Action: right  Reward: -1.0\n",
      "State: [28 39 13]  Action: stop  Reward: 0.2\n",
      "State: [21 47 33]  Action: right  Reward: 0.0\n",
      "State: [42 26 28]  Action: stop  Reward: 0.2\n",
      "State: [36  9 29]  Action: stop  Reward: -1.0\n",
      "State: [13 42 50]  Action: forward  Reward: 0.0\n",
      "State: [12 48 40]  Action: forward  Reward: 0.2\n",
      "State: [33 41 46]  Action: forward  Reward: 0.2\n",
      "State: [23 34 16]  Action: stop  Reward: 0.2\n",
      "State: [50 30  7]  Action: stop  Reward: -1.0\n",
      "State: [19 45  9]  Action: right  Reward: -1.0\n",
      "State: [13 28 26]  Action: stop  Reward: 0.2\n",
      "State: [28 28 14]  Action: right  Reward: 0.2\n",
      "State: [36 18 21]  Action: right  Reward: 0.0\n",
      "State: [48 25 39]  Action: stop  Reward: 0.2\n",
      "State: [37 29 23]  Action: forward  Reward: 0.0\n",
      "State: [19 24 26]  Action: stop  Reward: 0.0\n",
      "State: [ 8 31 38]  Action: right  Reward: -1.0\n",
      "State: [ 7 42 34]  Action: stop  Reward: -1.0\n",
      "State: [45 33 29]  Action: left  Reward: 0.2\n",
      "State: [ 8 25 46]  Action: left  Reward: -1.0\n",
      "State: [40 19 17]  Action: left  Reward: 0.0\n",
      "State: [18 34 21]  Action: forward  Reward: 0.2\n",
      "State: [49 19 42]  Action: right  Reward: 0.0\n",
      "State: [48 27 40]  Action: forward  Reward: 0.0\n",
      "State: [16 13 46]  Action: right  Reward: 0.0\n",
      "State: [29 42 11]  Action: forward  Reward: -1.0\n",
      "State: [20 32 21]  Action: stop  Reward: 0.0\n",
      "State: [33 46 43]  Action: stop  Reward: 0.0\n",
      "State: [32 36 35]  Action: forward  Reward: 0.0\n",
      "State: [ 8  8 35]  Action: left  Reward: -1.0\n",
      "State: [44 41 42]  Action: forward  Reward: 0.2\n",
      "State: [32 16 21]  Action: right  Reward: 0.0\n",
      "State: [42 28 31]  Action: stop  Reward: 0.2\n",
      "State: [ 5 31 13]  Action: stop  Reward: -1.0\n",
      "State: [12 46 14]  Action: forward  Reward: 0.2\n",
      "State: [27 48 24]  Action: stop  Reward: 0.2\n",
      "State: [40 10  9]  Action: stop  Reward: -1.0\n",
      "State: [41  8 21]  Action: stop  Reward: -1.0\n",
      "State: [22  7 19]  Action: left  Reward: -1.0\n",
      "State: [23 28 33]  Action: left  Reward: 0.0\n",
      "State: [39 40 37]  Action: stop  Reward: 0.2\n",
      "State: [ 6 25 42]  Action: right  Reward: -1.0\n",
      "State: [28 44 15]  Action: left  Reward: 0.0\n",
      "State: [44 14  9]  Action: left  Reward: -1.0\n",
      "State: [35  9 37]  Action: stop  Reward: -1.0\n",
      "State: [34 12  9]  Action: right  Reward: -1.0\n",
      "State: [30 23 26]  Action: forward  Reward: 0.0\n",
      "State: [ 9 15 23]  Action: right  Reward: -1.0\n",
      "State: [30 18 20]  Action: forward  Reward: 0.2\n",
      "State: [37 16 43]  Action: forward  Reward: 0.2\n",
      "State: [ 9 18 33]  Action: forward  Reward: -1.0\n",
      "State: [30 16 24]  Action: right  Reward: 0.0\n",
      "State: [25 34 30]  Action: forward  Reward: 0.2\n",
      "State: [37 19  6]  Action: forward  Reward: -1.0\n",
      "State: [37 10 29]  Action: forward  Reward: -1.0\n",
      "State: [43 29 46]  Action: right  Reward: 0.2\n",
      "State: [35 42 31]  Action: forward  Reward: 0.2\n",
      "State: [36 21 23]  Action: right  Reward: 0.2\n",
      "State: [ 5 21 37]  Action: right  Reward: -1.0\n",
      "State: [23  7 44]  Action: forward  Reward: -1.0\n",
      "State: [15 40 33]  Action: stop  Reward: 0.2\n",
      "State: [ 5 25 40]  Action: right  Reward: -1.0\n",
      "State: [29 17 10]  Action: right  Reward: -1.0\n",
      "State: [42 23 14]  Action: left  Reward: 0.0\n",
      "State: [50 23 15]  Action: forward  Reward: 0.0\n",
      "State: [20 45 37]  Action: right  Reward: 0.2\n",
      "State: [16 43 28]  Action: left  Reward: 0.0\n",
      "State: [42 13 40]  Action: stop  Reward: 0.0\n",
      "State: [26 38  5]  Action: forward  Reward: -1.0\n",
      "State: [25 12 27]  Action: forward  Reward: 0.2\n",
      "State: [22 32 21]  Action: left  Reward: 0.2\n",
      "State: [41 12 21]  Action: left  Reward: 0.2\n",
      "State: [45 31 49]  Action: right  Reward: 0.0\n",
      "State: [44 17 45]  Action: forward  Reward: 0.2\n",
      "State: [43 43 44]  Action: right  Reward: 0.0\n",
      "State: [ 9 17 26]  Action: right  Reward: -1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mState: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Peque침a pausa para simular \"flujo de datos\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import serial\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Configurar comunicaci칩n serial ---\n",
    "ser = serial.Serial('/dev/ttyUSB0', 115200)\n",
    "\n",
    "# --- Red neuronal peque침a ---\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')  # forward, left, right, stop\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# Tabla de acciones\n",
    "ACTIONS = [\"forward\", \"left\", \"right\", \"stop\"]\n",
    "\n",
    "def choose_action(state):\n",
    "    state = np.array(state).reshape(1, 3)\n",
    "    probs = model.predict(state, verbose=0)[0]\n",
    "    return np.random.choice(len(ACTIONS), p=probs)\n",
    "\n",
    "# --- Bucle de entrenamiento RL ---\n",
    "while True:\n",
    "    line = ser.readline().decode().strip()\n",
    "    data = json.loads(line)\n",
    "\n",
    "    state = np.array([data[\"L\"], data[\"C\"], data[\"R\"]])\n",
    "\n",
    "    action_index = choose_action(state)\n",
    "    action = ACTIONS[action_index]\n",
    "\n",
    "    ser.write((action + \"\\n\").encode())\n",
    "\n",
    "    # --- Definir recompensa ---\n",
    "    reward = 0\n",
    "    min_dist = min(state)\n",
    "\n",
    "    if min_dist < 12:\n",
    "        reward = -1.0\n",
    "    else:\n",
    "        reward = 0.2 if action == \"forward\" else 0.0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(np.array(state).reshape(1,3), training=True)\n",
    "        action_prob = logits[0, action_index]\n",
    "        loss = -tf.math.log(action_prob + 1e-8) * reward\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpex84r63y/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpex84r63y/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpex84r63y'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 3), dtype=tf.float32, name='keras_tensor_969')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  124838863490320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  124838863492048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  124838863493008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  124838863492240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  124838863493392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  124838863493200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1763122021.693826   66928 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1763122021.693852   66928 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-14 08:07:01.694219: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpex84r63y\n",
      "2025-11-14 08:07:01.694673: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-11-14 08:07:01.694680: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpex84r63y\n",
      "I0000 00:00:1763122021.698172   66928 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
      "2025-11-14 08:07:01.698875: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-11-14 08:07:01.726366: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpex84r63y\n",
      "2025-11-14 08:07:01.733237: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 39024 microseconds.\n",
      "2025-11-14 08:07:01.758445: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"robot_model.h5\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"robot.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
