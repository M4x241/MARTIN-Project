{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc753b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m s[\u001b[32m0\u001b[39m]*\u001b[32m4\u001b[39m + s[\u001b[32m1\u001b[39m]*\u001b[32m2\u001b[39m + s[\u001b[32m2\u001b[39m]*\u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     state = \u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mget_state\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     line = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.decode().strip()\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/6toSemestre/MARTIN-Project/.venv/lib/python3.12/site-packages/serial/serialposix.py:565\u001b[39m, in \u001b[36mSerial.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(read) < size:\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m         ready, _, _ = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe_abort_read_r\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipe_abort_read_r \u001b[38;5;129;01min\u001b[39;00m ready:\n\u001b[32m    567\u001b[39m             os.read(\u001b[38;5;28mself\u001b[39m.pipe_abort_read_r, \u001b[32m1000\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import serial\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ser = serial.Serial('/dev/ttyUSB1', 115200, timeout=1)\n",
    "\n",
    "ACTIONS = [0, 1, 2]   # forward, left, right\n",
    "\n",
    "def get_state():\n",
    "    line = ser.readline().decode().strip()\n",
    "    if not line:\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        return np.array([data[\"L\"], data[\"C\"], data[\"R\"]])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def send_action(a):\n",
    "    ser.write((str(a) + \"\\n\").encode())\n",
    "\n",
    "\n",
    "\n",
    "def reward_function(state):\n",
    "    L, C, R = state\n",
    "\n",
    "    # premio por avanzar sin obstaculos\n",
    "    if C == 0:\n",
    "        return 1.0\n",
    "\n",
    "    # penalizar si hay obst치culo de frente\n",
    "    if C == 1:\n",
    "        return -5.0\n",
    "\n",
    "    return -0.1\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "#  LOOP RL B츼SICO\n",
    "# -------------------------\n",
    "from random import choice\n",
    "\n",
    "Q = np.zeros((8, 3))  # 8 estados, 3 acciones\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "\n",
    "def state_to_index(s):\n",
    "    return s[0]*4 + s[1]*2 + s[2]*1\n",
    "\n",
    "while True:\n",
    "    state = get_state()\n",
    "    if state is None:\n",
    "        continue\n",
    "\n",
    "    idx = state_to_index(state)\n",
    "\n",
    "    # pol칤tica epsilon-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = choice(ACTIONS)\n",
    "    else:\n",
    "        action = np.argmax(Q[idx])\n",
    "\n",
    "    send_action(action)\n",
    "\n",
    "    time.sleep(0.15)\n",
    "\n",
    "    new_state = get_state()\n",
    "    if new_state is None:\n",
    "        continue\n",
    "\n",
    "    reward = reward_function(new_state)\n",
    "\n",
    "    new_idx = state_to_index(new_state)\n",
    "\n",
    "    # Q-learning\n",
    "    Q[idx, action] = Q[idx, action] + alpha * (\n",
    "        reward + gamma * np.max(Q[new_idx]) - Q[idx, action]\n",
    "    )\n",
    "\n",
    "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table cargada desde archivo.\n",
      "Enviando acci칩n inicial (avanzar = 0)...\n",
      "Comenzando entrenamiento...\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 1 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: -5.0\n",
      "State: [1 0 1] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [1 1 1] Action: 1 Reward: 1.0\n",
      "State: [0 0 1] Action: 0 Reward: 1.0\n",
      "State: [1 0 1] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 1 1] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 1 1] Action: 0 Reward: -5.0\n",
      "Q-table guardada.\n",
      "State: [0 1 1] Action: 0 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [1 1 0] Action: 0 Reward: -5.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: -5.0\n",
      "State: [1 1 0] Action: 0 Reward: -5.0\n",
      "State: [1 1 0] Action: 0 Reward: 1.0\n",
      "State: [0 1 0] Action: 0 Reward: -5.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 1 Reward: -5.0\n",
      "State: [0 0 0] Action: 0 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 1 1] Action: 0 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 1 0] Action: 2 Reward: -5.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 1 0] Action: 2 Reward: 1.0\n",
      "State: [1 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [1 0 0] Action: 1 Reward: 1.0\n",
      "State: [1 1 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 0 Reward: 1.0\n",
      "State: [0 0 1] Action: 1 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 1] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n",
      "Q-table guardada.\n",
      "State: [0 0 0] Action: 2 Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "import serial\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from random import choice\n",
    "\n",
    "# --- cargar Q-table ---\n",
    "if os.path.exists(\"qtable.npy\"):\n",
    "    Q = np.load(\"qtable.npy\")\n",
    "    print(\"Q-table cargada desde archivo.\")\n",
    "else:\n",
    "    Q = np.zeros((8, 3))\n",
    "    print(\"Q-table nueva creada.\")\n",
    "\n",
    "ser = serial.Serial('/dev/ttyUSB0', 115200, timeout=1)\n",
    "\n",
    "ACTIONS = [0, 1, 2]   # 0=avanzar, 1=izq, 2=der\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "\n",
    "\n",
    "def get_state():\n",
    "    try:\n",
    "        line = ser.readline().decode().strip()\n",
    "        if not line:\n",
    "            return None\n",
    "        data = json.loads(line)\n",
    "        return np.array([data[\"L\"], data[\"C\"], data[\"R\"]])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def send_action(a):\n",
    "    ser.write((str(a) + \"\\n\").encode())\n",
    "\n",
    "\n",
    "def reward_function(state):\n",
    "    L, C, R = state\n",
    "    if C == 0:\n",
    "        return 1.0\n",
    "    if C == 1:\n",
    "        return -5.0\n",
    "    return -0.1\n",
    "\n",
    "\n",
    "def state_to_index(s):\n",
    "    return s[0] * 4 + s[1] * 2 + s[2] * 1\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 游댠 ENVIAR ACCI칍N INICIAL PARA ARRANCAR\n",
    "# -------------------------------------------\n",
    "print(\"Enviando acci칩n inicial (avanzar = 0)...\")\n",
    "send_action(0)\n",
    "time.sleep(2)   # esperar para que el ESP32 empiece a enviar JSON\n",
    "print(\"Comenzando entrenamiento...\")\n",
    "\n",
    "\n",
    "last_save = time.time()\n",
    "\n",
    "# -------------------------------------------\n",
    "# 游대 BUCLE PRINCIPAL RL\n",
    "# -------------------------------------------\n",
    "while True:\n",
    "    state = get_state()\n",
    "    if state is None:\n",
    "        continue\n",
    "\n",
    "    idx = state_to_index(state)\n",
    "\n",
    "    # pol칤tica epsilon-greedy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = choice(ACTIONS)\n",
    "    else:\n",
    "        action = np.argmax(Q[idx])\n",
    "\n",
    "    send_action(action)\n",
    "    time.sleep(0.15)\n",
    "\n",
    "    new_state = get_state()\n",
    "    if new_state is None:\n",
    "        continue\n",
    "\n",
    "    reward = reward_function(new_state)\n",
    "    new_idx = state_to_index(new_state)\n",
    "\n",
    "    # actualizaci칩n Q-learning\n",
    "    Q[idx, action] = Q[idx, action] + alpha * (\n",
    "        reward + gamma * np.max(Q[new_idx]) - Q[idx, action]\n",
    "    )\n",
    "\n",
    "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward)\n",
    "\n",
    "    # guardado autom치tico\n",
    "    if time.time() - last_save > 5:\n",
    "        np.save(\"qtable.npy\", Q)\n",
    "        print(\"Q-table guardada.\")\n",
    "        last_save = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742760f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = np.load(\"qtable.npy\")\n",
    "Q\n",
    "\n",
    "Q_int = (Q * 1000).astype(int)  # opcional: escalar\n",
    "\n",
    "with open(\"qtable.h\", \"w\") as f:\n",
    "    f.write(\"const int Q[8][3] = {\\n\")\n",
    "    for row in Q_int:\n",
    "        f.write(\"  {\" + \", \".join(map(str, row)) + \"},\\n\")\n",
    "    f.write(\"};\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
